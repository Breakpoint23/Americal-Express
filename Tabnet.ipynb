{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp kaggle.json ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!kaggle competitions download -c amex-default-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!unzip amex-default-prediction.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cairo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print('Reading data...')\n",
    "chunk=pd.read_csv('train.csv',chunksize=100000)\n",
    "train_data=pd.concat(chunk)\n",
    "del chunk\n",
    "num_columns=[]\n",
    "cat_columns=[]\n",
    "NAN_THRESHOLD=0.9\n",
    "selected_columns=[\"S_2\"]\n",
    "print(\"Shape of train data:\",train_data.shape)\n",
    "\n",
    "samples=len(train_data)\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column==\"S_2\":\n",
    "        continue\n",
    "    #print(column,train_data[column].dtype)\n",
    "    nans=train_data[column].isnull().sum()/samples\n",
    "    if nans>NAN_THRESHOLD:\n",
    "        print(\"Column:\",column,\"has\",nans*100,\"% of NANs\")\n",
    "        #train_data[column].fillna(train_data[column].mean(),inplace=True)\n",
    "        #print(\"Filled with mean\")\n",
    "    else:\n",
    "        if train_data[column].dtype=='object':\n",
    "            train_data[column]=train_data[column].fillna(train_data[column].mode()[0])\n",
    "            cat_columns.append(column)\n",
    "            #print(\"Filled with mode\")\n",
    "        else:\n",
    "            train_data[column]=train_data[column].fillna(train_data[column].mean())\n",
    "            train_data[column]=train_data[column].round(decimals=2)\n",
    "            num_columns.append(column)\n",
    "            #print(\"Filled with mean\")\n",
    "        selected_columns.append(column)\n",
    "train_data=train_data[selected_columns]\n",
    "print(\"Shape of train data:\",train_data.shape)\n",
    "\n",
    "print(\"loading labels...\")\n",
    "train_labels=pd.read_csv('train_labels.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convert the Dates in S_2 column from string to Datetime\")\n",
    "train_data[\"S_2\"]=pd.to_datetime(train_data[\"S_2\"])\n",
    "\n",
    "gb=train_data.groupby(\"customer_ID\")\n",
    "indices=gb.indices\n",
    "\n",
    "\"\"\"\n",
    "We need to encode the object values into categorical first and then ordinally encode it\n",
    "We also need to round up the values to 2 decimals\n",
    "\"\"\"\n",
    "needed_cat_column=[\"customer_ID\",\"S_2\",\"target\"]\n",
    "for column in train_data.columns:\n",
    "  nan=train_data[column].isna().sum()\n",
    "  if nan/samples:\n",
    "    print(column,train_data[column].dtype,(nan/samples)*100)\n",
    "  if train_data[column].dtype==\"object\":\n",
    "    if column not in needed_cat_column:\n",
    "      train_data[column]=train_data[column].astype(\"category\").cat.codes.astype(float)\n",
    "  else:\n",
    "    try:\n",
    "      train_data[column]=train_data[column].round(decimals=2)\n",
    "    except:\n",
    "      print(column)\n",
    "\n",
    "\n",
    "train_data.D_64=train_data.D_64.fillna(train_data.D_64.mode())\n",
    "print(train_data.D_64.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import timedelta\n",
    "class TabnetDataset(Dataset):\n",
    "    def __init__(self, train_labels,map):\n",
    "        self.map=map\n",
    "        self.labels=train_labels\n",
    "        self.len=len(train_labels)\n",
    "        self.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cust_id,target=self.labels.iloc[idx][\"customer_ID\"],self.labels.iloc[idx][\"target\"]\n",
    "        indices=self.map[cust_id]\n",
    "        df=train_data.iloc[indices].sort_values(by=\"S_2\").drop(columns=[\"customer_ID\"])\n",
    "        df[\"S_2\"]=df[\"S_2\"].diff()/timedelta(minutes=1)\n",
    "        df[\"S_2\"]=df[\"S_2\"].astype(float)\n",
    "        df[\"S_2\"]=df[\"S_2\"].fillna(1).cumsum(axis=0)\n",
    "        for column in df.columns:\n",
    "            if column in num_columns:\n",
    "                df[f'{column}_wavg']=df[column].mul(df[\"S_2\"]).cumsum(axis=0)\n",
    "                df[f'{column}_wavg']=df[f'{column}_wavg']/df[\"S_2\"]\n",
    "        print(df.shape)\n",
    "        return torch.from_numpy(df.values).float().to(self.device),torch.from_numpy(np.array(target)).int().to(self.device)\n",
    "\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import dropout\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Predict_class_dense(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Predict_class_dense,self).__init__()\n",
    "    self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.LSTM=nn.LSTM(input_size=176,hidden_size=1500,num_layers=2,batch_first=True)\n",
    "    self.dense1=nn.Linear(in_features=1500,out_features=1800)\n",
    "    self.dense2=nn.Linear(in_features=1800,out_features=2000)\n",
    "    self.dense3=nn.Linear(in_features=2000,out_features=780)\n",
    "    self.dense4=nn.Linear(in_features=780,out_features=1)\n",
    "    self.relu=nn.ReLU()\n",
    "    self.softmax=nn.Softmax()\n",
    "    self.h_0=Variable(torch.zeros(2,1,1500)).to(self.device)\n",
    "\n",
    "    self.dropout1=nn.Dropout(p=0.2)\n",
    "    self.dropout2=nn.Dropout(p=0.3)\n",
    "    self.dropout3=nn.Dropout(p=0.2)\n",
    "  def forward(self,x):\n",
    "    x,_=self.LSTM(x,(self.h_0,self.h_0))\n",
    "    x=x[:,-1,:].reshape(-1,x.shape[2])\n",
    "    x=self.relu(x)\n",
    "    x=self.dense1(x)\n",
    "    x=self.relu(x)\n",
    "    x=self.dropout1(x)\n",
    "    x=self.dense2(x)\n",
    "    x=self.relu(x)\n",
    "    x=self.dropout2(x)\n",
    "    x=self.dense3(x)\n",
    "    x=self.relu(x)\n",
    "    x=self.dropout3(x)\n",
    "    x=self.dense4(x)\n",
    "    x=nn.Sigmoid(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Train_labels=train_labels.sample(frac=0.8)\n",
    "Test_labels=train_labels.drop(Train_labels.index).reset_index()\n",
    "Train_labels=Train_labels.reset_index()\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset=TabnetDataset(Train_labels,indices)\n",
    "test_dataset=TabnetDataset(Test_labels,indices)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "epochs=6\n",
    "model=Predict_class_dense().to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.00001)\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "test_loss=[]\n",
    "train_loss=[]\n",
    "test_accuracy=[]\n",
    "train_accuracy=[]\n",
    "for epoch in range(epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images=images.to(device)\n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        outputs=model(images)\n",
    "        #print(outputs,labels)\n",
    "        loss=criterion(outputs,labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch[{epoch+1}/{epochs}], loss: {loss.item()} \")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        tr_loss=[]\n",
    "        te_loss=[]\n",
    "        print(\"Testing on test data\")\n",
    "        true_labels=[]\n",
    "        final_output=[]\n",
    "        n_correct=0\n",
    "        n_samples=0\n",
    "\n",
    "\n",
    "        for images,labels in test_loader:\n",
    "            images=images.to(device)\n",
    "            labels=labels.type(torch.LongTensor)\n",
    "            labels=labels.to('cpu')\n",
    "            true_labels.append(labels)\n",
    "            labels=labels.to(device)\n",
    "            outputs=model(images)\n",
    "            \n",
    "            loss=criterion(outputs,labels)\n",
    "\n",
    "            _,predicted=torch.max(outputs.data,1)\n",
    "            n_samples+=labels.size(0)\n",
    "            n_correct+=(predicted==labels).sum().item()\n",
    "            predicted=predicted.cpu()\n",
    "            predicted=predicted.detach().numpy()\n",
    "            final_output.append(predicted)\n",
    "            te_loss.append(loss.item())\n",
    "        test_loss.append(np.mean(te_loss))\n",
    "        acc=100.0*n_correct/n_samples\n",
    "        acc=round(acc,4)\n",
    "        test_accuracy.append(acc)\n",
    "        print(f\"Testing accuracy for epoch {epoch+1}: {acc}\")\n",
    "        print(\"confusion matrix:\")\n",
    "        print(confusion_matrix(true_labels,final_output))\n",
    "\n",
    "        if epoch==0:\n",
    "            torch.save(model.state_dict(),f'test_model_{epoch+1}.h5')\n",
    "            max_acc=acc\n",
    "        else:\n",
    "            if acc>max_acc:\n",
    "                torch.save(model.state_dict(),f'test_model_{epoch+1}.h5')\n",
    "                max_acc=acc\n",
    "\n",
    "        true_labels=[]\n",
    "        final_output=[]\n",
    "        n_correct=0\n",
    "        n_samples=0\n",
    "\n",
    "        for images,labels in train_loader:\n",
    "            images=images.to(device)\n",
    "            labels=labels.type(torch.LongTensor)\n",
    "            labels=labels.to('cpu')\n",
    "            true_labels.append(labels)\n",
    "            labels=labels.to(device)\n",
    "            outputs=model(images)\n",
    "            \n",
    "            loss=criterion(outputs,labels)\n",
    "\n",
    "            _,predicted=torch.max(outputs.data,1)\n",
    "            n_samples+=labels.size(0)\n",
    "            n_correct+=(predicted==labels).sum().item()\n",
    "            predicted=predicted.cpu()\n",
    "            predicted=predicted.detach().numpy()\n",
    "            final_output.append(predicted)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        acc=100.0*n_correct/n_samples\n",
    "        acc=round(acc,4)\n",
    "        train_accuracy.append(acc)\n",
    "        print(f\"Training accuracy for epoch {epoch+1}: {acc}\")\n",
    "        print(\"confusion matrix:\")\n",
    "        print(confusion_matrix(true_labels,final_output))\n",
    "\n",
    "        if epoch==0:\n",
    "            torch.save(model.state_dict(),f'train_model_{epoch+1}.h5')\n",
    "            max_acc=acc\n",
    "        else:\n",
    "            if acc>max_acc:\n",
    "                torch.save(model.state_dict(),f'train_model_{epoch+1}.h5')\n",
    "                max_acc=acc\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0b5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
