{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlW8AoLig2Ij"
      },
      "outputs": [],
      "source": [
        "mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1PlrdHBg-nj"
      },
      "outputs": [],
      "source": [
        "cp kaggle.json ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjF0hbi-RM1w"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpHhfuy5hS0u",
        "outputId": "4c6e69ef-9062-41e7-d6c9-3485a54d3844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading amex-default-prediction.zip to /content\n",
            "100% 20.5G/20.5G [04:03<00:00, 96.0MB/s]\n",
            "100% 20.5G/20.5G [04:03<00:00, 90.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c amex-default-prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c7hJJQwhd5P",
        "outputId": "aae65164-98fc-4f52-cb91-64b49c2206d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  amex-default-prediction.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_data.csv           \n",
            "  inflating: train_data.csv          \n",
            "  inflating: train_labels.csv        \n"
          ]
        }
      ],
      "source": [
        "!unzip amex-default-prediction.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bJjj4KLzkNTw",
        "outputId": "d5758fa0-dc14-4ef2-d7eb-1c2a30ce73eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Loaded\n",
            "Looking for Nan Values\n",
            "D_73 float64 98.99021070601547\n",
            "D_87 float64 99.93012683290515\n",
            "D_88 float64 99.891457051685\n",
            "D_108 float64 99.4768461295237\n",
            "D_110 float64 99.43353018945662\n",
            "D_111 float64 99.43353018945662\n",
            "B_39 float64 99.39198593642065\n",
            "B_42 float64 98.70778933050298\n",
            "D_134 float64 96.48014598701137\n",
            "D_135 float64 96.48014598701137\n",
            "D_136 float64 96.48014598701137\n",
            "D_137 float64 96.48014598701137\n",
            "D_138 float64 96.48014598701137\n",
            "The nan theshold is set at:0.95\n",
            "D_73\n",
            "D_87\n",
            "D_88\n",
            "D_108\n",
            "D_110\n",
            "D_111\n",
            "B_39\n",
            "B_42\n",
            "D_134\n",
            "D_135\n",
            "D_136\n",
            "D_137\n",
            "D_138\n",
            "The original feature length is:177\n",
            "The selected feature lenght is:177\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import json\n",
        "chunk=pd.read_csv('train_data.csv',chunksize=100000)\n",
        "train_data=pd.concat(chunk)\n",
        "del chunk\n",
        "print('Data Loaded')\n",
        "samples=len(train_data)\n",
        "print('Looking for Nan Values')\n",
        "nans={}\n",
        "for column in train_data.columns:\n",
        "  if column==\"S_2\":\n",
        "    pass\n",
        "  else:\n",
        "    nan=train_data[column].isna().sum()\n",
        "    if nan/samples>=0.95:\n",
        "      print(column,train_data[column].dtype,(nan/samples)*100)\n",
        "\n",
        "    \n",
        "    if train_data[column].dtype==\"object\":\n",
        "        train_data[column]=train_data[column].fillna(train_data[column].mode())\n",
        "        #print(column,\"obj\")\n",
        "    else:\n",
        "        train_data[column]=train_data[column].fillna(train_data[column].mean())\n",
        "        #print(column,\"num\")\n",
        "  nans[column]=nan/samples\n",
        "\n",
        "\n",
        "#print('merging the training labels')\n",
        "train_labels=pd.read_csv('train_labels.csv')\n",
        "#training_data=pd.merge(train_data,train_labels,on='customer_ID')\n",
        "\n",
        "nan_threshold=0.95\n",
        "print(f'The nan theshold is set at:{nan_threshold}')\n",
        "selected_features=[]\n",
        "for key in nans.keys():\n",
        "  if nans[key]>nan_threshold:\n",
        "    print(key)\n",
        "    continue\n",
        "  else:\n",
        "    selected_features.append(key)\n",
        "\n",
        "#selected_features.append(\"S_2\")\n",
        "train_data=train_data[selected_features]\n",
        "features={\"f\":[]}\n",
        "for feature in selected_features:\n",
        "  features[\"f\"].append(feature)\n",
        "\n",
        "with open(\"Features.json\",\"w\") as f:\n",
        "  json.dump(features,f)\n",
        "print(f'The original feature length is:{len(train_data.columns)}')\n",
        "print(f'The selected feature lenght is:{len(selected_features)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-uHuRUKAasJ",
        "outputId": "2e977497-be74-437d-dc30-de422ec62893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Convert the Dates in S_2 column from string to Datetime\n",
            "S_2\n",
            "D_64 object 3.9310119532831442\n"
          ]
        }
      ],
      "source": [
        "print(\"Convert the Dates in S_2 column from string to Datetime\")\n",
        "train_data[\"S_2\"]=pd.to_datetime(train_data[\"S_2\"])\n",
        "\n",
        "gb=train_data.groupby(\"customer_ID\")\n",
        "indices=gb.indices\n",
        "\n",
        "\"\"\"\n",
        "We need to encode the object values into categorical first and then ordinally encode it\n",
        "We also need to round up the values to 2 decimals\n",
        "\"\"\"\n",
        "needed_cat_column=[\"customer_ID\",\"S_2\",\"target\"]\n",
        "for column in train_data.columns:\n",
        "  nan=train_data[column].isna().sum()\n",
        "  if nan/samples:\n",
        "    print(column,train_data[column].dtype,(nan/samples)*100)\n",
        "  if train_data[column].dtype==\"object\":\n",
        "    if column not in needed_cat_column:\n",
        "      train_data[column]=train_data[column].astype(\"category\").cat.codes.astype(float)\n",
        "  else:\n",
        "    try:\n",
        "      train_data[column]=train_data[column].round(decimals=2)\n",
        "    except:\n",
        "      print(column)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzxmdFcJs2JL",
        "outputId": "7ef92ff3-7ffa-4a71-aad3-eed02d53b262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "train_data.D_64=train_data.D_64.fillna(train_data.D_64.mode())\n",
        "print(train_data.D_64.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVtVLjEUl7Oh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "class Amex_dataset(Dataset):\n",
        "  def __init__(self,train_labels,map):\n",
        "    self.map=map\n",
        "    self.labels=train_labels\n",
        "    self.length=len(train_labels)\n",
        "    self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    \n",
        "    cust_id,target=self.labels.iloc[index][\"customer_ID\"],self.labels.iloc[index][\"target\"]\n",
        "    index=self.map[cust_id]\n",
        "    #print(index)\n",
        "    df=train_data.iloc[index].sort_values(\"S_2\").drop(columns=[\"customer_ID\"])\n",
        "    df[\"S_2\"]=df[\"S_2\"].diff()/timedelta(minutes=1)\n",
        "    df[\"S_2\"]=df[\"S_2\"].astype(float)\n",
        "    df[\"S_2\"]=df[\"S_2\"].fillna(0).cumsum(axis=0)\n",
        "    if len(df)>1:\n",
        "      df[\"S_2\"]=df[\"S_2\"]/df[\"S_2\"].max()\n",
        "    #print(\"here\")\n",
        "    #print(self.labels.iloc[index][\"target\"].to_numpy())\n",
        "    return torch.from_numpy(df.to_numpy()).float().to(self.device),torch.from_numpy(np.array(target)).int().to(self.device)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBpmpsyQx58i"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules import dropout\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Predict_class_dense(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Predict_class_dense,self).__init__()\n",
        "    self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.LSTM=nn.LSTM(input_size=176,hidden_size=1500,num_layers=2,batch_first=True)\n",
        "    self.dense1=nn.Linear(in_features=1500,out_features=1800)\n",
        "    self.dense2=nn.Linear(in_features=1800,out_features=2000)\n",
        "    self.dense3=nn.Linear(in_features=2000,out_features=780)\n",
        "    self.dense4=nn.Linear(in_features=780,out_features=1)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.softmax=nn.Softmax()\n",
        "    self.h_0=Variable(torch.zeros(2,1,1500)).to(self.device)\n",
        "\n",
        "    self.dropout1=nn.Dropout(p=0.2)\n",
        "    self.dropout2=nn.Dropout(p=0.3)\n",
        "    self.dropout3=nn.Dropout(p=0.2)\n",
        "  def forward(self,x):\n",
        "    x,_=self.LSTM(x,(self.h_0,self.h_0))\n",
        "    x=x[:,-1,:].reshape(-1,x.shape[2])\n",
        "    x=self.relu(x)\n",
        "    x=self.dense1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.dropout1(x)\n",
        "    x=self.dense2(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.dropout2(x)\n",
        "    x=self.dense3(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.dropout3(x)\n",
        "    x=self.dense4(x)\n",
        "    x=nn.Sigmoid(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oczu-mHkPaQp"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import ParamSpecKwargs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import json\n",
        "with open(\"Features.json\",\"r\") as f:\n",
        "  dic=json.load(f)\n",
        "\n",
        "selected_features=dic[\"f\"]\n",
        "sub_data_chunk=pd.read_csv(\"test_data.csv\",chunksize=100000)\n",
        "sub_data=pd.concat(sub_data_chunk)\n",
        "samples=len(sub_data)\n",
        "sub_data=sub_data[selected_features]\n",
        "\n",
        "for column in sub_data.columns:\n",
        "  if column==\"S_2\":\n",
        "    sub_data[\"S_2\"]=pd.to_datetime(sub_data[\"S_2\"])\n",
        "  elif column==\"customer_ID\":\n",
        "    pass\n",
        "  else:\n",
        "    if sub_data[column].dtype==\"object\":\n",
        "      sub_data[column]=sub_data[column].fillna(sub_data[column].mode()).astype(\"category\").cat.codes.astype(float)\n",
        "    else:\n",
        "      sub_data[column]=sub_data[column].fillna(sub_data[column].mean()).round(decimals=2)\n",
        "      \n",
        "sub_gb=sub_data.groupby(\"customer_ID\")\n",
        "map=sub_gb.indices\n",
        "\n",
        "\n",
        "for column in sub_data.columns:\n",
        "  nans=sub_data[column].isna().sum()\n",
        "  if nans:\n",
        "    sub_data[column]=sub_data[column].fillna(sub_data[column].mean())\n",
        "    print(column,nans/samples)\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIBP0NQo24Av",
        "outputId": "4b771e6c-3b05-4154-f9d3-ce3a6088390c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "customer_ID\n"
          ]
        }
      ],
      "source": [
        "for column in sub_data.columns:\n",
        "  if column==\"customer_ID\":\n",
        "    print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W9Yhl3apr6eT",
        "outputId": "584eaf4b-c1b1-41e1-b450-27ea2d96c1f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "percent complete:1.3706156360281672%\n",
            "percent complete:3.1307962938328244%\n",
            "percent complete:4.893140000064891%\n",
            "percent complete:6.636232575293012%\n",
            "percent complete:8.405065426807308%\n",
            "percent complete:10.16178520712811%\n",
            "percent complete:11.921208797983173%\n",
            "percent complete:13.671655737864489%\n",
            "percent complete:15.425347250386915%\n",
            "percent complete:17.18304040250005%\n",
            "percent complete:18.93943572555674%\n",
            "percent complete:20.687719617010643%\n",
            "percent complete:22.447900274815304%\n",
            "percent complete:24.20808093261996%\n",
            "percent complete:25.96901865737421%\n",
            "percent complete:27.732984649926834%\n",
            "percent complete:29.599046528253197%\n",
            "percent complete:31.389077254356106%\n",
            "percent complete:33.15228617995914%\n",
            "percent complete:34.928040786441144%\n",
            "percent complete:36.69817146701189%\n",
            "percent complete:38.46667986126208%\n",
            "percent complete:40.233998578877184%\n",
            "percent complete:42.00066838196407%\n",
            "percent complete:43.76268763093202%\n",
            "percent complete:45.53476505508743%\n",
            "percent complete:47.301002248488835%\n",
            "percent complete:49.07632424528536%\n",
            "percent complete:50.84807721217667%\n",
            "percent complete:52.61864050243289%\n",
            "percent complete:54.39223206048749%\n",
            "percent complete:56.158793711152995%\n",
            "percent complete:57.93206081194349%\n",
            "percent complete:59.69959583440134%\n",
            "percent complete:61.475350440883346%\n",
            "percent complete:63.250347980415754%\n",
            "percent complete:65.14939634726012%\n",
            "percent complete:66.92039224720183%\n",
            "percent complete:68.69084738503668%\n",
            "percent complete:70.46552046730498%\n",
            "percent complete:72.24257290284343%\n",
            "percent complete:74.01702968026899%\n",
            "percent complete:75.78672775115426%\n",
            "percent complete:77.56150898584393%\n",
            "percent complete:79.33499239147716%\n",
            "percent complete:81.1069616632112%\n",
            "percent complete:82.87709234378194%\n",
            "percent complete:84.64938607278009%\n",
            "percent complete:86.42860155674596%\n",
            "percent complete:88.20403170596384%\n",
            "percent complete:89.97870478823215%\n",
            "percent complete:91.75143112691579%\n",
            "percent complete:93.52707758097642%\n",
            "percent complete:95.29937130997457%\n",
            "percent complete:97.0728547156078%\n",
            "percent complete:98.84579735913418%\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6675152fd1e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission_1.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_ans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type Tensor is not JSON serializable"
          ]
        }
      ],
      "source": [
        "from torch._C import DeviceObjType\n",
        "from datetime import timedelta\n",
        "from torch.types import Device\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import json\n",
        "\n",
        "model=Predict_class_dense()\n",
        "model.load_state_dict(torch.load(\"model_3.h5\"))\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)\n",
        "saved_ans={\"customer_ID\":[],\"prediction\":[]}\n",
        "custom_n=len(map.keys())\n",
        "prev_time=time.time()\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  for i,key in enumerate(map.keys()):\n",
        "    if time.time()-prev_time>100:\n",
        "      print(f\"percent complete:{(i/custom_n)*100}%\")\n",
        "      prev_time=time.time()\n",
        "    inx=map[key]\n",
        "    df=sub_data.iloc[inx].sort_values(\"S_2\").drop(\"customer_ID\",axis=1)\n",
        "    df[\"S_2\"]=df[\"S_2\"].diff()/timedelta(minutes=1)\n",
        "    df[\"S_2\"]=df[\"S_2\"].astype(float)\n",
        "    df[\"S_2\"]=df[\"S_2\"].fillna(0).cumsum(axis=0)\n",
        "    if len(df)>1:\n",
        "      df[\"S_2\"]=df[\"S_2\"]/df[\"S_2\"].max()\n",
        "    images=torch.unsqueeze(torch.from_numpy(df.to_numpy()).float(),0).to(device)\n",
        "    outputs=model(images)\n",
        "    _,pred=torch.max(outputs.data,1)\n",
        "    saved_ans[\"customer_ID\"].append(key)\n",
        "    pred=pred.cpu().detach()\n",
        "    saved_ans[\"prediction\"].append(pred)\n",
        "\n",
        "with open(\"submission_1.json\",\"w\") as f:\n",
        "  json.dump(saved_ans,f)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eCkx0oZpkMd"
      },
      "outputs": [],
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "IKkvek81mYHs",
        "outputId": "09245bcc-5cb8-488d-8df3-5076416a2b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch[1/15], loss: 0.0023148665204644203 \n",
            "Testing accuracy for epoch 1: 89.5612\n",
            "Epoch[2/15], loss: 0.011056939139962196 \n",
            "Testing accuracy for epoch 2: 89.7574\n",
            "Epoch[3/15], loss: 1.0306141376495361 \n",
            "Testing accuracy for epoch 3: 89.6637\n",
            "Epoch[4/15], loss: 0.001956217223778367 \n",
            "Testing accuracy for epoch 4: 89.8445\n",
            "Epoch[5/15], loss: 0.5829675793647766 \n",
            "Testing accuracy for epoch 5: 89.6811\n",
            "Epoch[6/15], loss: 0.080205038189888 \n",
            "Testing accuracy for epoch 6: 89.3444\n",
            "Epoch[7/15], loss: 0.20785275101661682 \n",
            "Testing accuracy for epoch 7: 89.8227\n",
            "Epoch[8/15], loss: 0.0007753941463306546 \n",
            "Testing accuracy for epoch 8: 89.6342\n",
            "Epoch[9/15], loss: 0.4168844521045685 \n",
            "Testing accuracy for epoch 9: 89.7105\n",
            "Epoch[10/15], loss: 7.152531907195225e-06 \n",
            "Testing accuracy for epoch 10: 89.4991\n",
            "Epoch[11/15], loss: 0.7171515226364136 \n",
            "Testing accuracy for epoch 11: 89.7672\n",
            "Epoch[12/15], loss: 0.0023771857377141714 \n",
            "Testing accuracy for epoch 12: 89.7323\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8b28b3314296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.cuda import is_available\n",
        "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "\n",
        "\n",
        "Train_labels=train_labels.sample(frac=0.8)\n",
        "Test_labels=train_labels.drop(Train_labels.index).reset_index()\n",
        "Train_labels=Train_labels.reset_index()\n",
        "\n",
        "train_dataset=Amex_dataset(train_labels,indices)\n",
        "test_dataset=Amex_dataset(Test_labels,indices)\n",
        "torch.manual_seed(42)\n",
        "kfold=KFold(n_splits=5,shuffle=True)\n",
        "\n",
        "\n",
        "for fold,(train_index,test_index) in enumerate(kfold.split(train_dataset)):\n",
        "  train_subsampler=SubsetRandomSampler(train_index)\n",
        "  test_subsampler=SubsetRandomSampler(test_index)\n",
        "\n",
        "  train_loader=DataLoader(dataset=train_dataset,batch_size=1,shuffle=True,num_workers=0,sampler=train_subsampler)\n",
        "  test_loader=DataLoader(dataset=train_dataset,batch_size=1,shuffle=True,num_workers=0,sampler=test_subsampler)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  test_loss=[]\n",
        "  train_loss=[]\n",
        "  test_accuracy=[]\n",
        "  train_accuracy=[]\n",
        "  epochs=15\n",
        "  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  model=Predict_class_dense().to(device)\n",
        "  criterion=nn.BCEWithLogitsLoss()\n",
        "  optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i,(images,labels) in enumerate(train_loader):\n",
        "\n",
        "      images=images.to(device)\n",
        "      labels=labels.type(torch.LongTensor)\n",
        "      labels=labels.to(device)\n",
        "\n",
        "      outputs=model(images)\n",
        "      #print(outputs,labels)\n",
        "      loss=criterion(outputs,labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f\"Epoch[{epoch+1}/{epochs}], loss: {loss.item()} \")\n",
        "\n",
        "    with torch.no_grad():\n",
        "    #   model.eval()\n",
        "    #   saved_ans={\"customer_ID\":[],\"prediction\":[]}\n",
        "    #   for key in map.keys:\n",
        "    #     inx=map[key]\n",
        "    #     df=sub_data.iloc[inx].sort_values(\"S_2\").drop(\"customer_ID\")\n",
        "    #     df[\"S_2\"]=df[\"S_2\"].diff()/timedelta(minutes=1)\n",
        "    #     df[\"S_2\"]=df[\"S_2\"].astype(float)\n",
        "    #     df[\"S_2\"]=df[\"S_2\"].fillna(0).cumsum(axis=0)\n",
        "    #     if len(df)>1:\n",
        "    #       df[\"S_2\"]=df[\"S_2\"]/df[\"S_2\"].max()\n",
        "    #     images=torch.from_numpy(df.to_numpy()).float().to(device)\n",
        "    #     outputs=model(images)\n",
        "    #     _,pred=torch.max(outputs.data,1)\n",
        "    #     saved_ans[\"customer_ID\"].append(key)\n",
        "    #     pred=pred.cpu().detach()\n",
        "    #     saved_ans[\"prediction\"].append(pred)\n",
        "\n",
        "    #   with open(f\"Submission_file_{epoch}.csv\",\"wb\") as f:\n",
        "    #     writer=csv.DictWriter(f,fieldnames=[\"customer_ID\",\"prediction\"])\n",
        "    #     writer.writeheader()\n",
        "    #     writer.writerows(saved_ans)\n",
        "\n",
        "    #   print(f\"Submission_file_{epoch} saved\")\n",
        "\n",
        "      torch.save(model.state_dict(),f\"model_{epoch}.h5\")\n",
        "\n",
        "      train_loss=[]\n",
        "      test_loss=[]\n",
        "      true_labels=[]\n",
        "      final_output=[]\n",
        "      n_correct=0\n",
        "      n_samples=0\n",
        "      for images,labels in test_loader:\n",
        "        images=images.to(device)\n",
        "        labels=labels.type(torch.LongTensor)\n",
        "        labels=labels.to('cpu')\n",
        "        true_labels.append(labels)\n",
        "        labels=labels.to(device)\n",
        "        outputs=model(images)\n",
        "        \n",
        "        loss=criterion(outputs,labels)\n",
        "\n",
        "        _,predicted=torch.max(outputs.data,1)\n",
        "        n_samples+=labels.size(0)\n",
        "        n_correct+=(predicted==labels).sum().item()\n",
        "        predicted=predicted.cpu()\n",
        "        predicted=predicted.detach().numpy()\n",
        "        final_output.append(predicted)\n",
        "        test_loss.append(loss.item())\n",
        "\n",
        "      acc=100.0*n_correct/n_samples\n",
        "      acc=round(acc,4)\n",
        "      test_accuracy.append(acc)\n",
        "      print(f\"Testing accuracy for epoch {epoch+1}: {acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "6GxmvHITa-__",
        "outputId": "a3ee3664-4f83-4d42-e163-b847db52e8c7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9a344f28b672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data_chunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "test_data_chunk=pd.read_csv(\"test_data.csv\",chunksize=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3YbZiNmc85D"
      },
      "outputs": [],
      "source": [
        "test_data=pd.concat(test_data_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nqFKITUIEZO",
        "outputId": "99bcbb55-0b5f-4873-e159-4b0a690f79c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11363762\n"
          ]
        }
      ],
      "source": [
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "SNTY_rexKQXX",
        "outputId": "c436b2cd-38d2-4903-f266-0937683d4c2f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f80f5f69a113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
          ]
        }
      ],
      "source": [
        "test_data=test_data[selected_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "GqCbxMZpOX8v",
        "outputId": "5f901fe8-a02e-4991-b882-dbd52a9369cb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f80f5f69a113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
          ]
        }
      ],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz9KDfp9PJXH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "American_Express.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
